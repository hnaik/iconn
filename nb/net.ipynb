{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import matplotlib.pylab as plt                                                                                                                                                  \n",
    "import numpy as np                                                                                                                                                              \n",
    "import pandas as pd                                                                                                                                                            \n",
    "import torch                                                                                                                                                                 \n",
    "import torch.nn as nn                                                                                                                                                      \n",
    "import torch.optim as optim                                                                                                                                              \n",
    "import torchvision.transforms as transforms                                                                                                                        \n",
    "import sys                                                                                                                                                              \n",
    "                                                                                                                                                                         \n",
    "from argparse import ArgumentParser                                                                                                                                          \n",
    "from datetime import datetime                                                                                                                                          \n",
    "from pathlib import Path                                                                                                                                              \n",
    "from PIL import Image                                                                                                                                                     \n",
    "from sklearn import preprocessing                                                                                                                                          \n",
    "from sklearn.metrics import precision_recall_fscore_support                                                                                                               \n",
    "from sklearn.model_selection import train_test_split                                                                                                                        \n",
    "from torch.optim.lr_scheduler import StepLR                                                                                                                                 \n",
    "from torchvision import datasets, transforms                                                                                                                                 \n",
    "from torch.utils.data import Dataset, DataLoader                                                                                                                              \n",
    "from types import SimpleNamespace  \n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = '/home/hnaik/data/qualia/2d_shapes/64x64/circles/green/r18-c38_d34_t3.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "im = Image.open(im_path)\n",
    "X = trans(im)\n",
    "X = X.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hnaik/.virtualenvs/iconn-yLpHbsxh/lib/python3.8/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([1, 4])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=5, padding=2),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(3136, 4),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "label = torch.tensor([0., 1., 0., 0.])\n",
    "output = net(X)\n",
    "\n",
    "loss = criterion(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Flatten()\n",
       "  (7): Linear(in_features=3136, out_features=4, bias=True)\n",
       "  (8): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Flatten()\n",
       "  (7): Linear(in_features=3136, out_features=4, bias=True)\n",
       "  (8): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class IpAutograd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X):\n",
    "        ctx.save_for_backward(X)\n",
    "        ctx.template = 10\n",
    "        return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        X, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        print(grad_input.shape)\n",
    "        print(X.shape)\n",
    "        print(ctx.template)\n",
    "#         zt = 0\n",
    "#         for gi in grad_input:\n",
    "#             v, idx = gi.max(0)\n",
    "#             t = T[idx]\n",
    "#             zt += torch.exp((gi * t).sum())\n",
    "#         p_xt = 1 / zt * torch.exp(())\n",
    "        return grad_input\n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stage_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3)\n",
    "        \n",
    "        self.stage_2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)    Frame Title\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        X = self.max_pool(IpAutograd.apply(self.stage_1(X)))\n",
    "        X = self.max_pool(IpAutograd.apply(self.stage_2(X)))\n",
    "        return self.classifier(X)\n",
    "\n",
    "\n",
    "inet = Net()\n",
    "output = inet(X)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 21, 21])\n",
      "torch.Size([1, 64, 21, 21])\n",
      "10\n",
      "torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 32, 64, 64])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(output, label)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for im in output[0]:\n",
    "#     plt.imshow(im.detach().numpy(), cmap='gray')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365 64 21 21 tensor([ 5, 11, 23, 46, 12, 37, 29, 47, 53, 40, 18, 33, 27,  3, 39, 58, 43, 27,\n",
      "        42, 47, 16, 21,  9, 52, 20, 23, 63, 44,  3, 35,  1, 58, 32, 19, 22, 54,\n",
      "        36, 17, 43, 19, 18, 34,  4, 21, 31, 39, 37,  4, 60, 49, 46, 41, 39, 37,\n",
      "         3, 34, 54, 41, 52, 19,  0, 61,  6, 30])\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# z = np.random.rand(64 * 64).reshape([64, 64])\n",
    "# x = copy.deepcopy(output[0][0].detach().numpy())\n",
    "n = 64\n",
    "x = np.random.rand(n * n).reshape([n, n])\n",
    "x[n // 3, n // 3] = 100.0\n",
    "# z = output[0][0].detach().numpy()\n",
    "tau = 255\n",
    "beta = 0.25\n",
    "am = np.argmax(x)\n",
    "t_idx = torch.argmax(torch.from_numpy(x).squeeze(-1), dim=1)\n",
    "n = len(x[0])\n",
    "u_i = am // n\n",
    "u_j = am % n\n",
    "print(am, n, u_i, u_j, t_idx)\n",
    "u = (u_i, u_j)\n",
    "print(x[u_i, u_j])\n",
    "\n",
    "def norm_1(x1, x2):\n",
    "    return abs(x1[0] - x2[0]) + abs(x1[1] - x2[1])\n",
    "\n",
    "def norm_2(x1, x2):\n",
    "    return np.sqrt(math.pow((x1[0] - x2[0]), 2) + math.pow((x1[1] - x2[1]), 2))\n",
    "\n",
    "# u = (13, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAYAAACHjumMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASvUlEQVR4nO3dTZLbxhKFUciSHQ7vwtvR/ucaeBV2y9YbKcJdKiLzZuYFSL/vzNgE8cOWKoDbWVkfvn37dgCAw093nwCA/y4GGAA2DDAAbBhgANgwwACw+XT25ufPn9/9ienTp/eb//TTj+PT+rMPHz68e/3x48fT7Xf7XEX7WI8Z7XPdPrPP6PUV1r8A7v4iuP7sn3/+CT+jbr9u8/fff0v7XF/v9hFdx/r669evp5/P7GM9h2j7zHWox4yue/cZdR8T39Uff/yx/Q/AHQwAGwYYADYMMABsTjOYKHPZZRvdzCVzDDW3Wc8hk9F0M5ZMlqTaPX9HopxGzWR2ou+qct6rNVeIvt/13+6aM2T2MWG99uiY0e8jkxdG33f0f3T3XVX/P3AHA8CGAQaADQMMAJvTDCbKOnbPYVFuM1EXo+5DrYvJ7EN9Jq3UyazP3+t1TtTBRM/v6vP9cczUDEWfWXMCNYc7jjjXifKPaPvdcddjRv+WV7vvX81UHL+fR7iDAWDDAAPAhgEGgA0DDAAbKeRdg5010M18Zg2g1IA28xk18KsU2k1Mblz30W1fmgl5M5Pnzuyue/1ZFGRmvrsogI0K6TIBrHrMVSZoXo/bDZIzIW/3DxC7oDkKih/hDgaADQMMABsGGAA2pxmMWtC2+4w6YTJzjOnivczzZDRhclUpVopygsr7UfOn6LuZKKyrTHZUC+uigrZMcaX6/UbH3B23W7y3u471+50uvHv0swzuYADYMMAAsGGAAWBzmsFcUaMSbb/7m3y3aVXm7/7Rs/EVDajU7GKXIUTP9GrNyu66owxgQjdjqXw30T4yzaDUjCX6fVQymIkarmrNFncwAGwYYADYMMAAsGk1nNplF2qm4shg1FqcyrOzmslMNJyK3s/MU4nqKqJFuTLfleO76Lqr4VR3rlHmGNE+omNGuU/2PHa4gwFgwwADwIYBBoBNK4PJzBOarovZbdPNXDI5j1oX48gZKr1dom3UOpjMolyqSh+cTMPz6H01q4iOOVGjMpGPdOtimIsE4CUwwACwYYABYHOawawqi6I56mDUzzh6zkwvzLYTPfNPZDCVZ/7VxOL26jGifCSz6Hy0kN1EdqHW1kzkI1HtU6Wep/rvmTsYADYMMABsGGAA2DDAALC5veGUGgJnjqEGyZkAsBv6ToiCzEwxmVpYFzUN331GlZkkGAWyahPw49CL3tTQPXOM7jnsXLFoYLaBGncwAGwYYADYMMAAsBlvOKVmFer2mfPovs6c56sU2qmFdZnMJaIWvWWyo8y/vbN9VnIedYGySqGdWni3ozaxchTzPcIdDAAbBhgANgwwAGxOM5joOWui4VTULCpTB7O+jvZRaWrVzY4mMpio7iWTwUwstNb19vb27nXm+191F5DLbDPx3Uw3RK80g1InP1bO6xHuYADYMMAAsGGAAWAj1cFEzbYz+1hfRw26M8/njjqY7nymKzKY6PVxxNmEWveSmQsT1Xqsv/NdI3E1Y1Ezm8w+uwuxZY7RnZuUOS81B6pcx8N9pbYCgAIGGAA2DDAAbKSm35nnwek+Kpl5QmrGMlFr062T2f1MnWsU1bjsjtGtc6nM6YnmFWXm8EQZS9R4PNO7SK0PmViwTM15MtTPOGqdvuMOBoANAwwAGwYYADbSXKRMH041U4nen5jvpGYyu591F4ybyGDWjCWzuJhajxOdwy4LieZIRVnHbp9Rz931u1D782S2uWJukrp9JS+J+sNEx6xucxzcwQAwYoABYMMAA8CGAQaAjTTZMRP6douTJhZF6xbe7X6mhr6ZsDoSFdJFQedx9EPdTPNsdVE0R2Pr6JiZRdG6x6w0/Z4ocusGwZXvPzPp9Ti4gwFgxAADwIYBBoCNVGhXKV7qTgrMPNeqC8RlGk5FGYya0WSuIypYWzOXaGGwnW4j8UoxX2VBd3UioqMgzZGXqCauK9qH8zq5gwFgwwADwIYBBoBNa+G1iQlkap4ysY9K0+81Y/n5559PX2eaWkXPwldkLtHrK34fu8l33YzFsSiaur/MMdR8pHoek9srn+EOBoANAwwAGwYYADbjGYyq8qzdrbuozHeKMphffvnl9P1MzlOZa/Rvu+f1aD6Toy7JUaOyuqIht+O8HaJmXHcegzsYADYMMABsGGAA2EgLr1V6mjiaHE9nLpWF16JM5tdff333es1odvtcn2v/+uuvHz7zb5mF16Lr6GYyu5855s9E1MbWE67IaK7Iryq1NlncwQCwYYABYMMAA8BGymBWEz03rphT4qi1ibKNNXP57bfffjjGuk2Uuby9vZ0e01Gj4lhcLNPbOaqz6GY0x/FjTnNFhhK9f0VNy5W4gwFgwwADwIYBBoANAwwAm1bIu/MKE8Qq5zgd+h7Hcfz+++/vXn/58uXd6z///PN0n5mwdPUKv5/jeJ3zxDnuYADYMMAAsGGAAWAznsFkFku/W+Uc1UXRMhMX18xl3SbaZ9TQe+cVfj/H8TrniXPcwQCwYYABYMMAA8CmlcFUnoujLKNyjOh5XX2dOc9K5rJa61yifaiZzHHE1zHxXa3U32mlOZT6b6+ST0244rt4ZtzBALBhgAFgwwADwEbKYNbnw12z7JX6XJt55o+2URd43zX1ifKOdSH6aB7Q2izqOPSF19Z9rOdQuQ71u6r8PirZhiNjmXZFrY7jGFfWFHEHA8CGAQaADQMMAJvTDGaiRiXiqFHpZjLHEWcuUX+STF4VLYAVnUOUyez20c1cHLU2Feq/zUx2VNnHM7gif6oegzsYADYMMABsGGAA2IxnMOrzdyUfma572dWwqAvERflJZlE0df7TmrlkMpjpupjdeT5Dbc3EHDbVxHXcUefirFPiDgaADQMMABsGGAA2DDAAbKSQt1K8FAWqUbFZprBLDXUzC5apoW5UWJdZFG0VXUf0+jjiCZHd0Hf3s26wvzuPK8LSSpGhm2Pi6BUFtN9xBwPAhgEGgA0DDACb0wwmeibNFF1F7088n6/5RpS5VBZWj85zPWa0UP3uZ9F3E+UjmYZT05lMZpuJbCP6/tX3M8dV9zkxEbSiu8+JRu6PcAcDwIYBBoANAwwAG6kOZn2e3NV2RNuo+UmmRiXKXDITD1dqNqTmQLufdfOpStNvNXPZHUM9ryhb2m0TNTyqZB3Ti9A5sgxHfY96zOo2x8EdDAAjBhgANgwwAGykhdcqC3VHc4/WfUT5SWabTMbybxOLu63XqTasypzXRPPy6de740aZS6ZGJdpnZh/R++p8p2iflVobR53MFQ2nsriDAWDDAAPAhgEGgI00F2mV6XESZSzRgma77GKXA0SfOZN5do4yl6jHjCODyWRiapZRyWDUuUjr7zxz3it1nlYmL+zOPaoco9skf2Kfq8p1PMIdDAAbBhgANgwwAGxaC6/tntWirCHKWHaLh3VFz6C7henXbdQ6l0rPmegcKs/n6jyhiQwm6kGTOe9upnLHXKTqeXTfd+yzss0OdzAAbBhgANgwwACwYYABYCMV2mWCyijcjPYZbZ89jzOZkDFaOM1RWBe5YtKgGgJn9lFp7K42Wa9MBM000zo7p4lFAh1BcjcArwTij3AHA8CGAQaADQMMAJvTDEZdwKzymYmsQi0kWvOVzDOnmrk8S6Fdt3F4pem32ki8ko+oGc1EUzE1T8kcQz2HieK+iQZT2Ubi3MEAsGGAAWDDAAPARmr6namLydSxTFP/7r9uv5vs2K17uaIOZiKDUbON3WRUtflTpbamUksTva82/Z6og3EcozuhciJLeoQ7GAA2DDAAbBhgANi0mn5nMpjIRFax5iFvb2/vXn/69P4y1+13z5NR5qLOPZqog4nez9STqDlO1Cxq95nuvKHKZypzkbp1MI6G3GqeWPnMRM6TxR0MABsGGAA2DDAAbFoZjMPEXIs1H4kWd9vVwazbVBbZOjunDPUYlToMtQ5morH4HfObdseYXmhtIueZ6PlzRc5DHQyA2zHAALBhgAFgwwADwEZqOLWqNJyaoE5ejELgTGC17lMtrNt9l1GQvKq8rzaydoSlauib+cxEw6lusV6lQK27QmUm+L9ihcos7mAA2DDAALBhgAFgc5rBREVZuxwiKnLbFbWdqSyKtlrPO/P5KB+ZyJbUDCYy0dg6ymyuyEsm9lGZCKpmLpWcZ7rwLrPP7oJy2W12uIMBYMMAA8CGAQaAjTTZMcpXjiNu7qQ+D+7yEXUhtSgv2Z3T9MJqVzScqtRhrCoZQHfCZKW2JmqEVWlq5cinpidU7kw3Eo+uc/eZR7iDAWDDAAPAhgEGgE2r4dQuH4nqXqKalUyu0K17ibKl3XHVDKXSYCpSaQDWrbvIPI87GnJ361zuaDiVmQum5h+VRdG6dS/UwQB4CQwwAGwYYADYnGYwa56y1rjsqHUvkcxcpG5ekplTpdbBXNEwfaIOJnqWrjSdVutFHLU1dywYV6mDUX8fE/OdJupeyGAA3I4BBoANAwwAG6kfTKa3i9r/Vu2fexzx87nay2WXwai9WaLcx7HwWqV/jPp8PrHw1x19fiv9YNSalcp1dPu/ZK4jOgf1OrPb7HAHA8CGAQaADQMMABsGGAA2rYZTlSK6aB/RImrHoYe40eTHXUCrNpCKtncU3k0soNWdKHcc/ebYE0VwVzScUhdN222jTmZ0LO5WKearNMI6Du5gABgxwACwYYABYDPecEp93o4mFe6eB9dtokxlYhG16Dwjjqbfme3VIqxKBqD+zicWRXuVYj41J5u4jm4jLQrtALwEBhgANgwwAGxOM5hKTYQ6qU/NZHbbrJMw18+ozaN2P4ueOSsZS9dE3cvEZMeVmpdU6mAcC9M/Q63NHQuvTVzHI9zBALBhgAFgwwADwKZVB5PJHaLnv0qNitpAqpKfqHUvr5LBdOelTDQWn2hi1c02dtvcsbjbMzTOmsh9HuEOBoANAwwAGwYYADbjc5GixuBqU/CJDEZ9vTuGypHJdM9pt4+JuUjduS+ZOTzdxdwy2cV0XpLZx8T3P71P5iIBeAkMMABsGGAA2LQymMxcpPU5NapRydSfqP1cJuYiTb9fUemlGm3jqImYmPvSnSOVqVFZ3dEPRt3n7jruyJLIYADcjgEGgA0DDAAbBhgANlLDqcyiaN2m3tFib7t9RMeIguadqHHWqzT9XlWC++j97uTGieK9icXdnnFCZfR/cKcb3Gd+H9lFF7mDAWDDAAPAhgEGgI1UaBcV0e22UTOZysJr6uTHaH+Pjqvs8w4ThXaV7e+YwDfR1ErNbSqLyndznEqu1s19KLQD8BIYYADYMMAAsBlv+q029b5jUbTM4nDTkxmfpQ4m2qbyrN3NXCrNoCYmVEb/vruLvWXOq1t7k9mn+jozoTKLOxgANgwwAGwYYADYjM9FivKOaJ+ZhlPdpt4TzbPVTCWT+6zU5947moJXPlPJeRxzkf4r852m9zkxp+077mAA2DDAALBhgAFgc5rBVJ7N1IXU1Cbhu8+oNSvqXKXdNmrtTbZ/xplKxtKda1Spg1Ezlomm32pNS2WflXqe7nwnxzHUzCazj0e4gwFgwwADwIYBBoCNNBdpzRUy8yLWvETNaCZ6tUz0171i7lHkigxmdcfcpN1x1T6ymexC7TNbmYukZkNqj97dZ9R9OH4f33EHA8CGAQaADQMMABsGGAA20mTHykTEKByKQuBKADsxufGKyYxdlSZA3SZWmSKs6P1KYOhoODW9z8x1dJtYVSZtdov9stvscAcDwIYBBoANAwwAG3vDqeh19EyayXnUQrqJwjrHZMauiabfE82H1H1MNLHKZBUr9bxfpalVd/Lj7hiV7/c4uIMBYMQAA8CGAQaAjdRwavX169cffrZmE2tOM9GgO8px1MxlYiG2Z1Vp9nT2+atynm7GMjGBr1sns9umm484Gk5NTKh8hDsYADYMMABsGGAA2LQaTmVyiTWniWpYMsdQc5voPDPzKv6rDacmPt+ts8hkF91jVnKFievoNg53XIdaN5PZ5yPcwQCwYYABYMMAA8DmNINZ85OopmX3syiLiBZaq9SoqPOIMtuouU43+6hw1Khk3n/GxuGVuTNqHqI2DZ/YZyUfmWj6TT8YAE+HAQaADQMMABtpLlJU03IcuZzm7P1KP93uXCPHwmvPopu5VPrlRu9PzG/qzk3KnEd3rlJmm+48osx5OY5BBgPgdgwwAGwYYADYMMAAsGlNdtyJguCJyY6r6DOVhtwsvJb/fDfkrSyKpm4/EcBGn68sWKY2/LqrqRUhL4CnwwADwIYBBoDNeMMpdRJg9Gy921+U26jntHtfPa9XWXhN/YxjkuDEMbqvM+c5cQy1IDA6p4mmVpVcqJL3HQd3MACMGGAA2DDAALD5cEdjJAD/H7iDAWDDAAPAhgEGgA0DDAAbBhgANgwwAGz+Bw767TPkznKLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAYAAACHjumMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAADnElEQVR4nO3ZMQqAMBAAQSP5/5fjB4TYrILMlLnmquUgY611ABTOrxcA/ktggIzAABmBATICA2TmZu6LCXhi3D26YICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwACZuZmPV7YAfskFA2QEBsgIDJARGCAjMEBGYIDMBY3XBjLcklm9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-61d743172835>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_neg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "T_1 = np.zeros([n, n])\n",
    "T_2 = np.zeros([n, n])\n",
    "T_neg = np.zeros([n, n])\n",
    "\n",
    "def compute(p, u, norm_func):\n",
    "    x = 0.005 - beta * norm_func(p, u) / n\n",
    "    return np.sqrt(tau * np.max(0.005 - x, -1))\n",
    "    \n",
    "    \n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        T_1[i, j] = compute((i, j), u, norm_1)\n",
    "        T_2[i, j] = compute((i, j), u, norm_2)\n",
    "  \n",
    "T_neg.fill(-tau)\n",
    "\n",
    "def show(im):\n",
    "    plt.imshow(im, cmap='gray_r')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show(T_1)\n",
    "show(T_2)\n",
    "show(T_neg)\n",
    "show(T_1 * x)\n",
    "show(T_2 * x)\n",
    "show(T_neg * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = x * T_1\n",
    "\n",
    "plt.imshow(xx, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in inet.parameters():\n",
    "    print(x.shape)\n",
    "    \n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4],\n",
       "        [6, 8]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[2, 2], [2, 2]])\n",
    "\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-08788eb07ddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "x = torch.zeros([3, 3])\n",
    "\n",
    "x.assign(-tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassB 10\n",
      "ClassA 100\n"
     ]
    }
   ],
   "source": [
    "class ClassA:\n",
    "    @staticmethod\n",
    "    def helper(X):\n",
    "        print(f'ClassA {X*X}')\n",
    "        \n",
    "    \n",
    "class ClassB(ClassA):\n",
    "    @staticmethod\n",
    "    def helper(X):\n",
    "        print(f'ClassB {X}')\n",
    "        ClassA.helper(X)\n",
    "        \n",
    "class ClassC(ClassB):\n",
    "    ...\n",
    "        \n",
    "ClassC.helper(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
